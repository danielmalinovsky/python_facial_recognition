{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import src.proprietary_functions as src\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of samples strings\n",
    "list_samples = ['train','valid','test']\n",
    "#dictionary for storing cropped images, image names and labels\n",
    "arr_dicts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing numpy objects of cropped images\n",
    "for sampl in list_samples:\n",
    "    with open(f'./cropped_numpys/cropped_{sampl}_X.npy', 'rb') as f:\n",
    "        arr_dicts[f'arr_{sampl}_X']  = np.load(f)\n",
    "\n",
    "arr_train_X = arr_dicts['arr_train_X']\n",
    "arr_valid_X = arr_dicts['arr_valid_X']\n",
    "arr_test_X = arr_dicts['arr_test_X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing numpy objects of image names\n",
    "for sampl in list_samples:\n",
    "    with open(f'./cropped_numpys/cropped_{sampl}_X_names.npy', 'rb') as f:\n",
    "        arr_dicts[f'{sampl}_X_names']  = np.load(f)\n",
    "\n",
    "train_X_names = arr_dicts['train_X_names']\n",
    "valid_X_names = arr_dicts['valid_X_names']\n",
    "test_X_names = arr_dicts['test_X_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing numpy objects of labels\n",
    "for sampl in list_samples:\n",
    "    with open(f'./cropped_numpys/cropped_{sampl}_Y.npy', 'rb') as f:\n",
    "        arr_dicts[f'arr_{sampl}_Y']  = np.load(f)\n",
    "\n",
    "arr_train_Y = arr_dicts['arr_train_Y']\n",
    "arr_valid_Y = arr_dicts['arr_valid_Y']\n",
    "arr_test_Y = arr_dicts['arr_test_Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for creation of triplets\n",
    "def\tmake_triplets(images, labels, image_names):\n",
    "\n",
    "\ttripletImages = []\n",
    "\ttripletImagesNames = []\n",
    "\tuniqueClasses = np.unique(labels)\n",
    "\n",
    "\tdict_idx = {i:np.where(labels == i)[0] for i in uniqueClasses}\n",
    "\n",
    "\tfor idxA in range(len(images)):\n",
    "\n",
    "\t\t#current image\n",
    "\t\tcurrentImage = images[idxA]\n",
    "\t\tlabel = labels[idxA]\n",
    "\t\tcurrentImage_name = image_names[idxA]\n",
    "\n",
    "\t\t#positive image\n",
    "\t\tidxB = np.random.choice(dict_idx[label])\n",
    "\t\tposImage = images[idxB]\n",
    "\t\tposImage_name = image_names[idxB]\n",
    "\n",
    "\t\t#negative image\n",
    "\t\tnegLab = np.random.choice([i for i in dict_idx.keys() if i != label])\n",
    "\t\tnegIdx = np.random.choice(dict_idx[negLab])\n",
    "\t\tnegImage = images[negIdx]\n",
    "\t\tnegImage_name = image_names[negIdx]\n",
    "        \n",
    "\t\t#saving the triplets of images and image names\n",
    "\t\ttripletImages.append([currentImage, posImage, negImage])\n",
    "\t\ttripletImagesNames.append([currentImage_name, posImage_name, negImage_name])\n",
    "\n",
    "\treturn (np.array(tripletImages), np.array(tripletImagesNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_triplets, train_triplets_names= make_triplets(arr_train_X, arr_train_Y, train_X_names)\n",
    "valid_X_triplets, valid_triplets_names = make_triplets(arr_valid_X, arr_valid_Y, valid_X_names)\n",
    "test_X_triplets, test_triplets_names = make_triplets(arr_test_X, arr_test_Y, test_X_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./triplet_numpys/'):\n",
    "    os.makedirs('./triplet_numpys/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_triplets = {'train': {'triplet_imgs': train_X_triplets, 'triplet_imgs_names':train_triplets_names},\n",
    "                'valid': {'triplet_imgs': valid_X_triplets,'triplet_imgs_names':valid_triplets_names},\n",
    "                'test': {'triplet_imgs': test_X_triplets, 'triplet_imgs_names':test_triplets_names}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporting the triplets of images and image names as numpy objects.\n",
    "for sampl in dict_triplets.keys():\n",
    "    for n in dict_triplets[sampl].keys():\n",
    "       with open(f'./triplet_numpys/{sampl}_{n}.npy', 'wb') as f:\n",
    "            np.save(f, dict_triplets[sampl][n])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN model building with triplet loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_base_network():\n",
    "    input = Input(shape=(224,224,3))\n",
    "    x = Flatten()(input)\n",
    "    x = Dense(120, activation='relu')(x)\n",
    "    return Model(inputs=input, outputs=x)\n",
    "\n",
    "embedding = initialize_base_network()\n",
    "tf.keras.utils.plot_model(embedding, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNet(tf.keras.layers.Layer):\n",
    "    # set the backbone model in constructor\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def call(self, feat):\n",
    "        # get feature vectors from anchor\n",
    "        feats = self.model(feat[0])\n",
    "        # from positive image\n",
    "        pfeats = self.model(feat[1])\n",
    "        # and from negative image\n",
    "        nfeats = self.model(feat[2])\n",
    "        # concatenate vectors to a matrix\n",
    "        result = tf.stack([feats, pfeats, nfeats])\n",
    "        return result\n",
    "\n",
    "class TripletLoss(tf.keras.layers.Layer):\n",
    "    # margin is settable hyperparameter in constructor\n",
    "    def __init__(self, margin):\n",
    "        self.margin = margin\n",
    "        super().__init__()\n",
    "        \n",
    "    # function calculating distance between features\n",
    "    def distance(self, x, y):\n",
    "        sum_square = tf.reduce_sum(tf.square(x - y), axis=1, keepdims=True)\n",
    "        return tf.sqrt(tf.maximum(sum_square, tf.keras.backend.epsilon()))\n",
    "\n",
    "    def call(self, features):\n",
    "        # get anchor-positive distance\n",
    "        pos = self.distance(features[0], features[1])\n",
    "        # anchor-negative distance\n",
    "        neg = self.distance(features[0], features[2])\n",
    "        # difference between anchor positive and anchor negative distances\n",
    "        loss = pos - neg\n",
    "        # get overall loss\n",
    "        return tf.maximum(loss + self.margin, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anchor branch\n",
    "image_input = Input(shape=(224,224,3), name='image_input')\n",
    "# positive image branch\n",
    "positive_input = Input(shape=(224,224,3), name='positive_input')\n",
    "# negative image branch\n",
    "negative_input = Input(shape=(224,224,3), name='negative_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(abs(y_true-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese = SiameseNet(embedding)([image_input, positive_input, negative_input])\n",
    "loss = TripletLoss(margin=1.0)(siamese)\n",
    "model = Model(inputs=[image_input, positive_input, negative_input], outputs=loss)\n",
    "model.compile(optimizer ='adam', loss = identity_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 861. MiB for an array with shape (6000, 224, 224, 3) and data type uint8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ngnpe\\OneDrive\\Desktop\\python_facial_recognition\\Sprint5_TripletLoss.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ngnpe/OneDrive/Desktop/python_facial_recognition/Sprint5_TripletLoss.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit([train_X_triplets[:,\u001b[39m0\u001b[39;49m], train_X_triplets[:,\u001b[39m1\u001b[39;49m], train_X_triplets[:,\u001b[39m2\u001b[39;49m]], np\u001b[39m.\u001b[39;49mones(train_X_triplets\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ngnpe/OneDrive/Desktop/python_facial_recognition/Sprint5_TripletLoss.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                 verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ngnpe/OneDrive/Desktop/python_facial_recognition/Sprint5_TripletLoss.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m             validation_data\u001b[39m=\u001b[39;49m([valid_X_triplets[:,\u001b[39m0\u001b[39;49m], valid_X_triplets[:,\u001b[39m1\u001b[39;49m], valid_X_triplets[:,\u001b[39m2\u001b[39;49m]], np\u001b[39m.\u001b[39;49mones(valid_X_triplets\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m])),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ngnpe/OneDrive/Desktop/python_facial_recognition/Sprint5_TripletLoss.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m             epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1133\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1127\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cluster_coordinator \u001b[39m=\u001b[39m cluster_coordinator\u001b[39m.\u001b[39mClusterCoordinator(\n\u001b[0;32m   1128\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy)\n\u001b[0;32m   1130\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy\u001b[39m.\u001b[39mscope(), \\\n\u001b[0;32m   1131\u001b[0m      training_utils\u001b[39m.\u001b[39mRespectCompiledTrainableState(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1132\u001b[0m   \u001b[39m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[1;32m-> 1133\u001b[0m   data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[0;32m   1134\u001b[0m       x\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m   1135\u001b[0m       y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m   1136\u001b[0m       sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1137\u001b[0m       batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1138\u001b[0m       steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   1139\u001b[0m       initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[0;32m   1140\u001b[0m       epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m   1141\u001b[0m       shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1142\u001b[0m       class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[0;32m   1143\u001b[0m       max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1144\u001b[0m       workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1145\u001b[0m       use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1146\u001b[0m       model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1147\u001b[0m       steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution)\n\u001b[0;32m   1149\u001b[0m   \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   1150\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1364\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1362\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m_cluster_coordinator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1363\u001b[0m   \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 1364\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1154\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1152\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m   1153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verify_data_adapter_compatibility(adapter_cls)\n\u001b[1;32m-> 1154\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[0;32m   1155\u001b[0m     x,\n\u001b[0;32m   1156\u001b[0m     y,\n\u001b[0;32m   1157\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1158\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   1159\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[0;32m   1160\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1161\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1162\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1163\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1164\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1165\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mds_context\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[0;32m   1166\u001b[0m     model\u001b[39m=\u001b[39;49mmodel)\n\u001b[0;32m   1168\u001b[0m strategy \u001b[39m=\u001b[39m ds_context\u001b[39m.\u001b[39mget_strategy()\n\u001b[0;32m   1170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:247\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[0;32m    237\u001b[0m              x,\n\u001b[0;32m    238\u001b[0m              y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    244\u001b[0m              shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    245\u001b[0m              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    246\u001b[0m   \u001b[39msuper\u001b[39m(TensorLikeDataAdapter, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(x, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 247\u001b[0m   x, y, sample_weights \u001b[39m=\u001b[39m _process_tensorlike((x, y, sample_weights))\n\u001b[0;32m    248\u001b[0m   sample_weight_modes \u001b[39m=\u001b[39m broadcast_sample_weight_modes(\n\u001b[0;32m    249\u001b[0m       sample_weights, sample_weight_modes)\n\u001b[0;32m    251\u001b[0m   \u001b[39m# If sample_weights are not specified for an output use 1.0 as weights.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1046\u001b[0m, in \u001b[0;36m_process_tensorlike\u001b[1;34m(inputs)\u001b[0m\n\u001b[0;32m   1043\u001b[0m     \u001b[39mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n\u001b[0;32m   1044\u001b[0m   \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m-> 1046\u001b[0m inputs \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39;49mmap_structure(_convert_numpy_and_scipy, inputs)\n\u001b[0;32m   1047\u001b[0m \u001b[39mreturn\u001b[39;00m nest\u001b[39m.\u001b[39mlist_to_tuple(inputs)\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:867\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    863\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    864\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    866\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 867\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    868\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:867\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    863\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    864\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    866\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 867\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    868\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1041\u001b[0m, in \u001b[0;36m_process_tensorlike.<locals>._convert_numpy_and_scipy\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1039\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(x\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, np\u001b[39m.\u001b[39mfloating):\n\u001b[0;32m   1040\u001b[0m     dtype \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39mfloatx()\n\u001b[1;32m-> 1041\u001b[0m   \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mconvert_to_tensor_v2_with_dispatch(x, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m   1042\u001b[0m \u001b[39melif\u001b[39;00m _is_scipy_sparse(x):\n\u001b[0;32m   1043\u001b[0m   \u001b[39mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 206\u001b[0m   \u001b[39mreturn\u001b[39;00m target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    207\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m    208\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m    209\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m   result \u001b[39m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1430\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconvert_to_tensor\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[0;32m   1367\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[0;32m   1368\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_to_tensor_v2_with_dispatch\u001b[39m(\n\u001b[0;32m   1369\u001b[0m     value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype_hint\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1370\u001b[0m   \u001b[39m\"\"\"Converts the given `value` to a `Tensor`.\u001b[39;00m\n\u001b[0;32m   1371\u001b[0m \n\u001b[0;32m   1372\u001b[0m \u001b[39m  This function converts Python objects of various types to `Tensor`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1428\u001b[0m \u001b[39m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001b[39;00m\n\u001b[0;32m   1429\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1430\u001b[0m   \u001b[39mreturn\u001b[39;00m convert_to_tensor_v2(\n\u001b[0;32m   1431\u001b[0m       value, dtype\u001b[39m=\u001b[39;49mdtype, dtype_hint\u001b[39m=\u001b[39;49mdtype_hint, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1436\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1434\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_to_tensor_v2\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype_hint\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1435\u001b[0m   \u001b[39m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1436\u001b[0m   \u001b[39mreturn\u001b[39;00m convert_to_tensor(\n\u001b[0;32m   1437\u001b[0m       value\u001b[39m=\u001b[39;49mvalue,\n\u001b[0;32m   1438\u001b[0m       dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1439\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m   1440\u001b[0m       preferred_dtype\u001b[39m=\u001b[39;49mdtype_hint,\n\u001b[0;32m   1441\u001b[0m       as_ref\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:163\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[0;32m    162\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1566\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1561\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mconvert_to_tensor did not convert to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1562\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mthe preferred dtype: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m vs \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1563\u001b[0m                       (ret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype, preferred_dtype\u001b[39m.\u001b[39mbase_dtype))\n\u001b[0;32m   1565\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1566\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[0;32m   1568\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[0;32m   1569\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:52\u001b[0m, in \u001b[0;36m_default_conversion_function\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_default_conversion_function\u001b[39m(value, dtype, name, as_ref):\n\u001b[0;32m     51\u001b[0m   \u001b[39mdel\u001b[39;00m as_ref  \u001b[39m# Unused.\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m   \u001b[39mreturn\u001b[39;00m constant_op\u001b[39m.\u001b[39;49mconstant(value, dtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:264\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    168\u001b[0m   \u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \n\u001b[0;32m    170\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    265\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:276\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    275\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 276\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m    278\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n\u001b[0;32m    279\u001b[0m tensor_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue()\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:301\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[0;32m    300\u001b[0m   \u001b[39m\"\"\"Implementation of eager constant.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 301\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[0;32m    302\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    303\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m     97\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 861. MiB for an array with shape (6000, 224, 224, 3) and data type uint8"
     ]
    }
   ],
   "source": [
    "history = model.fit([train_X_triplets[:,0], train_X_triplets[:,1], train_X_triplets[:,2]], np.ones(train_X_triplets.shape[0]),\n",
    "                verbose=1,\n",
    "            validation_data=([valid_X_triplets[:,0], valid_X_triplets[:,1], valid_X_triplets[:,2]], np.ones(valid_X_triplets.shape[0])),\n",
    "            epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(H):\n",
    "\t# construct a plot that plots and saves the training history\n",
    "\tplt.style.use(\"ggplot\")\n",
    "\tplt.figure()\n",
    "\n",
    "\tplt.plot(H.history[\"accuracy\"], label=\"train_acc\")\n",
    "\tplt.plot(H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "\tplt.title(\"Training Accuracy\")\n",
    "\tplt.xlabel(\"Epoch #\")\n",
    "\tplt.ylabel(\"Accuracy\")\n",
    "\tplt.legend()\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "\tplt.plot(H.history[\"loss\"], label=\"train_loss\")\n",
    "\tplt.plot(H.history[\"val_loss\"], label=\"val_loss\")\n",
    "\tplt.title(\"Training Loss\")\n",
    "\tplt.xlabel(\"Epoch #\")\n",
    "\tplt.ylabel(\"Loss\")\n",
    "\tplt.legend()\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VSE_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf7e70c757e4f60095653c44545a762e49c6e5d3353dc968e17e829e1045004e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
