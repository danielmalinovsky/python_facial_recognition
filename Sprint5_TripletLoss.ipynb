{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import src.proprietary_functions as src\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of samples strings\n",
    "list_samples = ['train','valid','test']\n",
    "#dictionary for storing cropped images, image names and labels\n",
    "arr_dicts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing numpy objects of cropped images\n",
    "for sampl in list_samples:\n",
    "    with open(f'./cropped_numpys/cropped_{sampl}_X.npy', 'rb') as f:\n",
    "        arr_dicts[f'arr_{sampl}_X']  = np.load(f)\n",
    "\n",
    "arr_train_X = arr_dicts['arr_train_X']\n",
    "arr_valid_X = arr_dicts['arr_valid_X']\n",
    "arr_test_X = arr_dicts['arr_test_X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing numpy objects of image names\n",
    "for sampl in list_samples:\n",
    "    with open(f'./cropped_numpys/cropped_{sampl}_X_names.npy', 'rb') as f:\n",
    "        arr_dicts[f'{sampl}_X_names']  = np.load(f)\n",
    "\n",
    "train_X_names = arr_dicts['train_X_names']\n",
    "valid_X_names = arr_dicts['valid_X_names']\n",
    "test_X_names = arr_dicts['test_X_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing numpy objects of labels\n",
    "for sampl in list_samples:\n",
    "    with open(f'./cropped_numpys/cropped_{sampl}_Y.npy', 'rb') as f:\n",
    "        arr_dicts[f'arr_{sampl}_Y']  = np.load(f)\n",
    "\n",
    "arr_train_Y = arr_dicts['arr_train_Y']\n",
    "arr_valid_Y = arr_dicts['arr_valid_Y']\n",
    "arr_test_Y = arr_dicts['arr_test_Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for creation of triplets\n",
    "def\tmake_triplets(images, labels, image_names):\n",
    "\n",
    "\ttripletImages = []\n",
    "\ttripletImagesNames = []\n",
    "\tuniqueClasses = np.unique(labels)\n",
    "\n",
    "\tdict_idx = {i:np.where(labels == i)[0] for i in uniqueClasses}\n",
    "\n",
    "\tfor idxA in range(len(images)):\n",
    "\n",
    "\t\t#current image\n",
    "\t\tcurrentImage = images[idxA]\n",
    "\t\tlabel = labels[idxA]\n",
    "\t\tcurrentImage_name = image_names[idxA]\n",
    "\n",
    "\t\t#positive image\n",
    "\t\tidxB = np.random.choice(dict_idx[label])\n",
    "\t\tposImage = images[idxB]\n",
    "\t\tposImage_name = image_names[idxB]\n",
    "\n",
    "\t\t#negative image\n",
    "\t\tnegLab = np.random.choice([i for i in dict_idx.keys() if i != label])\n",
    "\t\tnegIdx = np.random.choice(dict_idx[negLab])\n",
    "\t\tnegImage = images[negIdx]\n",
    "\t\tnegImage_name = image_names[negIdx]\n",
    "        \n",
    "\t\t#saving the triplets of images and image names\n",
    "\t\ttripletImages.append([currentImage, posImage, negImage])\n",
    "\t\ttripletImagesNames.append([currentImage_name, posImage_name, negImage_name])\n",
    "\n",
    "\treturn (np.array(tripletImages), np.array(tripletImagesNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_triplets, train_triplets_names= make_triplets(arr_train_X, arr_train_Y, train_X_names)\n",
    "valid_X_triplets, valid_triplets_names = make_triplets(arr_valid_X, arr_valid_Y, valid_X_names)\n",
    "test_X_triplets, test_triplets_names = make_triplets(arr_test_X, arr_test_Y, test_X_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./triplet_numpys/'):\n",
    "    os.makedirs('./triplet_numpys/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_triplets = {'train': {'triplet_imgs': train_X_triplets, 'triplet_imgs_names':train_triplets_names},\n",
    "                'valid': {'triplet_imgs': valid_X_triplets,'triplet_imgs_names':valid_triplets_names},\n",
    "                'test': {'triplet_imgs': test_X_triplets, 'triplet_imgs_names':test_triplets_names}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporting the triplets of images and image names as numpy objects.\n",
    "for sampl in dict_triplets.keys():\n",
    "    for n in dict_triplets[sampl].keys():\n",
    "       with open(f'./triplet_numpys/{sampl}_{n}.npy', 'wb') as f:\n",
    "            np.save(f, dict_triplets[sampl][n])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN model building with triplet loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_base_network():\n",
    "    input = Input(shape=(224,224,3))\n",
    "    x = Flatten()(input)\n",
    "    x = Dense(120, activation='relu')(x)\n",
    "    return Model(inputs=input, outputs=x)\n",
    "\n",
    "embedding = initialize_base_network()\n",
    "tf.keras.utils.plot_model(embedding, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNet(tf.keras.layers.Layer):\n",
    "    # set the backbone model in constructor\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def call(self, feat):\n",
    "        # get feature vectors from anchor\n",
    "        feats = self.model(feat[0])\n",
    "        # from positive image\n",
    "        pfeats = self.model(feat[1])\n",
    "        # and from negative image\n",
    "        nfeats = self.model(feat[2])\n",
    "        # concatenate vectors to a matrix\n",
    "        result = tf.stack([feats, pfeats, nfeats])\n",
    "        return result\n",
    "\n",
    "class TripletLoss(tf.keras.layers.Layer):\n",
    "    # margin is settable hyperparameter in constructor\n",
    "    def __init__(self, margin):\n",
    "        self.margin = margin\n",
    "        super().__init__()\n",
    "        \n",
    "    # function calculating distance between features\n",
    "    def distance(self, x, y):\n",
    "        sum_square = tf.reduce_sum(tf.square(x - y), axis=1, keepdims=True)\n",
    "        return tf.sqrt(tf.maximum(sum_square, tf.keras.backend.epsilon()))\n",
    "\n",
    "    def call(self, features):\n",
    "        # get anchor-positive distance\n",
    "        pos = self.distance(features[0], features[1])\n",
    "        # anchor-negative distance\n",
    "        neg = self.distance(features[0], features[2])\n",
    "        # difference between anchor positive and anchor negative distances\n",
    "        loss = pos - neg\n",
    "        # get overall loss\n",
    "        return tf.maximum(loss + self.margin, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anchor branch\n",
    "image_input = Input(shape=(224,224,3), name='image_input')\n",
    "# positive image branch\n",
    "positive_input = Input(shape=(224,224,3), name='positive_input')\n",
    "# negative image branch\n",
    "negative_input = Input(shape=(224,224,3), name='negative_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(abs(y_true-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese = SiameseNet(embedding)([image_input, positive_input, negative_input])\n",
    "loss = TripletLoss(margin=1.0)(siamese)\n",
    "model = Model(inputs=[image_input, positive_input, negative_input], outputs=loss)\n",
    "model.compile(optimizer ='adam', loss = identity_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 861. MiB for an array with shape (6000, 224, 224, 3) and data type uint8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ngnpe\\OneDrive\\Desktop\\python_facial_recognition\\Sprint5_TripletLoss.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ngnpe/OneDrive/Desktop/python_facial_recognition/Sprint5_TripletLoss.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit([train_X_triplets[:,\u001b[39m0\u001b[39;49m], train_X_triplets[:,\u001b[39m1\u001b[39;49m], train_X_triplets[:,\u001b[39m2\u001b[39;49m]], np\u001b[39m.\u001b[39;49mones(train_X_triplets\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ngnpe/OneDrive/Desktop/python_facial_recognition/Sprint5_TripletLoss.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                 verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ngnpe/OneDrive/Desktop/python_facial_recognition/Sprint5_TripletLoss.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m             validation_data\u001b[39m=\u001b[39;49m([valid_X_triplets[:,\u001b[39m0\u001b[39;49m], valid_X_triplets[:,\u001b[39m1\u001b[39;49m], valid_X_triplets[:,\u001b[39m2\u001b[39;49m]], np\u001b[39m.\u001b[39;49mones(valid_X_triplets\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m])),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ngnpe/OneDrive/Desktop/python_facial_recognition/Sprint5_TripletLoss.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m             epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1133\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1127\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cluster_coordinator \u001b[39m=\u001b[39m cluster_coordinator\u001b[39m.\u001b[39mClusterCoordinator(\n\u001b[0;32m   1128\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy)\n\u001b[0;32m   1130\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy\u001b[39m.\u001b[39mscope(), \\\n\u001b[0;32m   1131\u001b[0m      training_utils\u001b[39m.\u001b[39mRespectCompiledTrainableState(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1132\u001b[0m   \u001b[39m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[1;32m-> 1133\u001b[0m   data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[0;32m   1134\u001b[0m       x\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m   1135\u001b[0m       y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m   1136\u001b[0m       sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1137\u001b[0m       batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1138\u001b[0m       steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   1139\u001b[0m       initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[0;32m   1140\u001b[0m       epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m   1141\u001b[0m       shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1142\u001b[0m       class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[0;32m   1143\u001b[0m       max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1144\u001b[0m       workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1145\u001b[0m       use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1146\u001b[0m       model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1147\u001b[0m       steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution)\n\u001b[0;32m   1149\u001b[0m   \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   1150\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1364\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1362\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m_cluster_coordinator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1363\u001b[0m   \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 1364\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1154\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1152\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m   1153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verify_data_adapter_compatibility(adapter_cls)\n\u001b[1;32m-> 1154\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[0;32m   1155\u001b[0m     x,\n\u001b[0;32m   1156\u001b[0m     y,\n\u001b[0;32m   1157\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1158\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   1159\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[0;32m   1160\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1161\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1162\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1163\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1164\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1165\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mds_context\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[0;32m   1166\u001b[0m     model\u001b[39m=\u001b[39;49mmodel)\n\u001b[0;32m   1168\u001b[0m strategy \u001b[39m=\u001b[39m ds_context\u001b[39m.\u001b[39mget_strategy()\n\u001b[0;32m   1170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:247\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[0;32m    237\u001b[0m              x,\n\u001b[0;32m    238\u001b[0m              y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    244\u001b[0m              shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    245\u001b[0m              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    246\u001b[0m   \u001b[39msuper\u001b[39m(TensorLikeDataAdapter, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(x, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 247\u001b[0m   x, y, sample_weights \u001b[39m=\u001b[39m _process_tensorlike((x, y, sample_weights))\n\u001b[0;32m    248\u001b[0m   sample_weight_modes \u001b[39m=\u001b[39m broadcast_sample_weight_modes(\n\u001b[0;32m    249\u001b[0m       sample_weights, sample_weight_modes)\n\u001b[0;32m    251\u001b[0m   \u001b[39m# If sample_weights are not specified for an output use 1.0 as weights.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1046\u001b[0m, in \u001b[0;36m_process_tensorlike\u001b[1;34m(inputs)\u001b[0m\n\u001b[0;32m   1043\u001b[0m     \u001b[39mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n\u001b[0;32m   1044\u001b[0m   \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m-> 1046\u001b[0m inputs \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39;49mmap_structure(_convert_numpy_and_scipy, inputs)\n\u001b[0;32m   1047\u001b[0m \u001b[39mreturn\u001b[39;00m nest\u001b[39m.\u001b[39mlist_to_tuple(inputs)\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:867\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    863\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    864\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    866\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 867\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    868\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:867\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    863\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    864\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    866\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 867\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    868\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1041\u001b[0m, in \u001b[0;36m_process_tensorlike.<locals>._convert_numpy_and_scipy\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1039\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(x\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, np\u001b[39m.\u001b[39mfloating):\n\u001b[0;32m   1040\u001b[0m     dtype \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39mfloatx()\n\u001b[1;32m-> 1041\u001b[0m   \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mconvert_to_tensor_v2_with_dispatch(x, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m   1042\u001b[0m \u001b[39melif\u001b[39;00m _is_scipy_sparse(x):\n\u001b[0;32m   1043\u001b[0m   \u001b[39mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 206\u001b[0m   \u001b[39mreturn\u001b[39;00m target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    207\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m    208\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m    209\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m   result \u001b[39m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1430\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconvert_to_tensor\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[0;32m   1367\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[0;32m   1368\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_to_tensor_v2_with_dispatch\u001b[39m(\n\u001b[0;32m   1369\u001b[0m     value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype_hint\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1370\u001b[0m   \u001b[39m\"\"\"Converts the given `value` to a `Tensor`.\u001b[39;00m\n\u001b[0;32m   1371\u001b[0m \n\u001b[0;32m   1372\u001b[0m \u001b[39m  This function converts Python objects of various types to `Tensor`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1428\u001b[0m \u001b[39m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001b[39;00m\n\u001b[0;32m   1429\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1430\u001b[0m   \u001b[39mreturn\u001b[39;00m convert_to_tensor_v2(\n\u001b[0;32m   1431\u001b[0m       value, dtype\u001b[39m=\u001b[39;49mdtype, dtype_hint\u001b[39m=\u001b[39;49mdtype_hint, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1436\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1434\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_to_tensor_v2\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype_hint\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1435\u001b[0m   \u001b[39m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1436\u001b[0m   \u001b[39mreturn\u001b[39;00m convert_to_tensor(\n\u001b[0;32m   1437\u001b[0m       value\u001b[39m=\u001b[39;49mvalue,\n\u001b[0;32m   1438\u001b[0m       dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1439\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m   1440\u001b[0m       preferred_dtype\u001b[39m=\u001b[39;49mdtype_hint,\n\u001b[0;32m   1441\u001b[0m       as_ref\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:163\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[0;32m    162\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1566\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1561\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mconvert_to_tensor did not convert to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1562\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mthe preferred dtype: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m vs \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1563\u001b[0m                       (ret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype, preferred_dtype\u001b[39m.\u001b[39mbase_dtype))\n\u001b[0;32m   1565\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1566\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[0;32m   1568\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[0;32m   1569\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:52\u001b[0m, in \u001b[0;36m_default_conversion_function\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_default_conversion_function\u001b[39m(value, dtype, name, as_ref):\n\u001b[0;32m     51\u001b[0m   \u001b[39mdel\u001b[39;00m as_ref  \u001b[39m# Unused.\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m   \u001b[39mreturn\u001b[39;00m constant_op\u001b[39m.\u001b[39;49mconstant(value, dtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:264\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    168\u001b[0m   \u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \n\u001b[0;32m    170\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    265\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:276\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    275\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 276\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m    278\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n\u001b[0;32m    279\u001b[0m tensor_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue()\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:301\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[0;32m    300\u001b[0m   \u001b[39m\"\"\"Implementation of eager constant.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 301\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[0;32m    302\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    303\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "File \u001b[1;32mc:\\Users\\ngnpe\\anaconda3\\envs\\VSE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m     97\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 861. MiB for an array with shape (6000, 224, 224, 3) and data type uint8"
     ]
    }
   ],
   "source": [
    "history = model.fit([train_X_triplets[:,0], train_X_triplets[:,1], train_X_triplets[:,2]], np.ones(train_X_triplets.shape[0]),\n",
    "                verbose=1,\n",
    "            validation_data=([valid_X_triplets[:,0], valid_X_triplets[:,1], valid_X_triplets[:,2]], np.ones(valid_X_triplets.shape[0])),\n",
    "            epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function creating pairs\n",
    "def\tmake_pairs(images, labels):\n",
    "\n",
    "\tpairImages = []\n",
    "\tpairLabels = []\n",
    "\tuniqueClasses = np.unique(labels)\n",
    "\n",
    "\tdict_idx = {i:np.where(labels == i)[0] for i in uniqueClasses}\n",
    "\n",
    "\tfor idxA in range(len(images)):\n",
    "\t\tcurrentImage = images[idxA]\n",
    "\t\tlabel = labels[idxA]\n",
    "\n",
    "\t\t#positive pair\n",
    "\t\tidxB = np.random.choice(dict_idx[label])\n",
    "\t\tposImage = images[idxB]\n",
    "\t\tpairImages.append([currentImage, posImage])\n",
    "\n",
    "\t\tpairLabels.append([1])\n",
    "\n",
    "\t\t#negative pair\n",
    "\t\tnegLab = np.random.choice([i for i in dict_idx.keys() if i != label])\n",
    "\t\tnegIdx = np.random.choice(dict_idx[negLab])\n",
    "\t\tnegImage = images[negIdx]\n",
    "\t\tpairImages.append([currentImage, negImage])\n",
    "\n",
    "\t\tpairLabels.append([0])\n",
    "\n",
    "\treturn (np.array(pairImages),np.array(pairLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairTest, labelTest = make_pairs(arr_test_X, arr_test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_pair = pairTest[:,0]\n",
    "right_pair = pairTest[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_pair_pred = embedding.predict(left_pair)\n",
    "right_pair_pred = embedding.predict(right_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(left_pred, right_pred, y_true):\n",
    "    y_pred = np.linalg.norm(left_pred - right_pred, axis=1)\n",
    "#     # 1 for the same - distance is smaller than 3.0, 0 for the different\n",
    "    pred = y_pred < 7.0\n",
    "    return np.mean(pred == y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = compute_accuracy(left_pair_pred, right_pair_pred, labelTest)\n",
    "print(f'Test accuracy: {test_accuracy*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VSE_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf7e70c757e4f60095653c44545a762e49c6e5d3353dc968e17e829e1045004e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
